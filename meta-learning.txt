# ğŸ”¬ Complete Pipeline: Meta-Learning + Multi-Objective Optimization for Flow-Level Graph Classification

---

## Executive Summary

**Goal:** Build a line graph from 19M network flows where each flow is a node, classify into 15 classes with emphasis on rare attack detection (SQL Injection: 440, XSS: 480, Web Brute Force: 1,618 samples).

**Core Innovation:** 
- Meta-learned edge selection that optimizes for few-shot rare class performance
- Multi-objective optimization for edge budget allocation across edge types
- Class-adaptive graph density (rare classes get denser connectivity)

**Compute:** 8Ã— A100 GPUs (cloud)

**Priority:** Rare class recall > Overall F1 > Computational efficiency

---

# Phase 0: Problem Formulation & Experimental Design

## 0.1 Formal Problem Definition

```
Given:
- Dataset D = {(x_i, y_i)}_{i=1}^{N} where N â‰ˆ 19M
- x_i âˆˆ â„^d is flow feature vector (d â‰ˆ 55 original + engineered features)
- y_i âˆˆ {1, 2, ..., 15} is attack class label
- Extreme class imbalance: majority class 17.5M, minority classes 440-1,618

Construct:
- Graph G = (V, E) where V = D (each flow is a node)
- Edge set E selected by learned policy Ï€_Î¸
- Edge budget B = [B_1, B_2, ..., B_K] across K edge types

Optimize:
- Primary: Rare class recall (classes with < 5,000 samples)
- Secondary: Macro-F1 across all 15 classes
- Tertiary: Graph sparsity (computational efficiency)
```

## 0.2 Class Stratification

Define class tiers based on sample count for differential treatment:

| Tier | Classes | Sample Range | Treatment |
|------|---------|--------------|-----------|
| **Tier 1: Extreme Minority** | SQL_Injection, XSS | 440-480 | Maximum edge density (k=100), meta-learning focus |
| **Tier 2: Minority** | Brute_Force_Web, LOIC-UDP | 1,618-3,450 | High edge density (k=50) |
| **Tier 3: Moderate** | Slowloris, GoldenEye, Hulk, SlowHTTPTest | 36K-106K | Medium edge density (k=30) |
| **Tier 4: Common Attacks** | Infiltration, SSH, Bot, LOIC-HTTP, FTP, HOIC | 188K-1M | Standard edge density (k=15) |
| **Tier 5: Majority** | Benign | 17.5M | Sparse edges (k=5) |

## 0.3 Experimental Design

**Research Questions:**
1. Does meta-learned edge selection outperform heuristic edge construction?
2. Does multi-objective optimization find better sparsity-accuracy trade-offs?
3. How does class-adaptive edge density affect rare class recall?
4. Which edge types contribute most to each attack type?

**Baselines for Comparison:**
1. **B1:** Random k-NN edges (k=20, uniform)
2. **B2:** Feature-similarity k-NN edges (cosine similarity)
3. **B3:** Temporal edges only (same session, time window)
4. **B4:** Heuristic multi-type edges (your original 4 types, fixed ratios)
5. **B5:** Attention-based edge learning (GAT with dense initial graph)

**Ablation Studies:**
- A1: Meta-learning ON vs OFF
- A2: Multi-objective vs single objective (accuracy only)
- A3: Class-adaptive density vs uniform density
- A4: Each edge type individually

---

# Phase 1: Data Preparation & Feature Engineering

## 1.1 Feature Categories

Organize features into semantic groups for edge construction:

```
IDENTITY FEATURES (for structural edges):
â”œâ”€â”€ IPV4_SRC_ADDR, IPV4_DST_ADDR
â”œâ”€â”€ L4_SRC_PORT, L4_DST_PORT
â”œâ”€â”€ PROTOCOL, L7_PROTO
â””â”€â”€ Derived: src_subnet_24, dst_subnet_24, 5-tuple hash

TEMPORAL FEATURES (for temporal edges):
â”œâ”€â”€ FLOW_START_MILLISECONDS, FLOW_END_MILLISECONDS
â”œâ”€â”€ FLOW_DURATION_MILLISECONDS
â”œâ”€â”€ DURATION_IN, DURATION_OUT
â””â”€â”€ Derived: hour_of_day, day_of_week, is_business_hours

VOLUME FEATURES (for behavioral similarity):
â”œâ”€â”€ IN_BYTES, OUT_BYTES, IN_PKTS, OUT_PKTS
â”œâ”€â”€ Derived ratios: bytes_in_out_ratio, pkts_per_second
â””â”€â”€ Throughput: SRC_TO_DST_AVG_THROUGHPUT, DST_TO_SRC_AVG_THROUGHPUT

IAT FEATURES (for behavioral fingerprinting):
â”œâ”€â”€ SRC_TO_DST_IAT_MIN/MAX/AVG/STDDEV
â”œâ”€â”€ DST_TO_SRC_IAT_MIN/MAX/AVG/STDDEV
â””â”€â”€ Derived: iat_coefficient_of_variation, iat_regularity

TCP FEATURES (for protocol behavior):
â”œâ”€â”€ TCP_FLAGS, CLIENT_TCP_FLAGS, SERVER_TCP_FLAGS
â”œâ”€â”€ TCP_WIN_MAX_IN, TCP_WIN_MAX_OUT
â”œâ”€â”€ RETRANSMITTED_IN/OUT_BYTES/PKTS
â””â”€â”€ Derived: has_syn, has_ack, has_rst, is_complete_handshake

PACKET SIZE DISTRIBUTION:
â”œâ”€â”€ NUM_PKTS_UP_TO_128_BYTES through NUM_PKTS_1024_TO_1514_BYTES
â”œâ”€â”€ LONGEST_FLOW_PKT, SHORTEST_FLOW_PKT
â””â”€â”€ Derived: small_pkt_ratio, large_pkt_ratio, pkt_size_entropy

APPLICATION FEATURES:
â”œâ”€â”€ DNS_QUERY_ID, DNS_QUERY_TYPE, DNS_TTL_ANSWER
â”œâ”€â”€ ICMP_TYPE, ICMP_IPV4_TYPE
â””â”€â”€ Derived: is_dns_query, is_icmp_echo
```

## 1.2 Feature Normalization Strategy

```
Per-feature normalization based on distribution:

HEAVY-TAILED (bytes, packets, duration):
â”œâ”€â”€ Apply log1p transformation: x' = log(1 + x)
â”œâ”€â”€ Then robust scaling: (x' - median) / IQR
â””â”€â”€ Handles extreme outliers (e.g., LOIC-UDP with 7M avg bytes)

BOUNDED (ratios, flags):
â”œâ”€â”€ Min-max scaling to [0, 1]
â””â”€â”€ Preserve interpretability

CATEGORICAL (protocol, port categories):
â”œâ”€â”€ One-hot encoding for low cardinality (protocol: 6 values)
â”œâ”€â”€ Target encoding for high cardinality (L7_PROTO: 149 values)
â””â”€â”€ Embedding lookup for very high cardinality (ports)

TEMPORAL (timestamps):
â”œâ”€â”€ Convert to relative time from dataset start
â”œâ”€â”€ Cyclical encoding for hour/day: sin(2Ï€ Ã— t / period), cos(2Ï€ Ã— t / period)
â””â”€â”€ Preserve raw milliseconds for temporal edge construction
```

## 1.3 Data Splitting Strategy

```
TEMPORAL SPLIT (respects time ordering):
â”œâ”€â”€ Training: First 70% of time span (Feb 14 - Feb 25)
â”œâ”€â”€ Validation: Next 15% (Feb 25 - Feb 28)
â””â”€â”€ Test: Final 15% (Feb 28 - Mar 2)

STRATIFICATION:
â”œâ”€â”€ Ensure all 15 classes present in each split
â”œâ”€â”€ For Tier 1 classes (440-480 samples): use 60-20-20 split
â””â”€â”€ Apply stratified sampling within each temporal window

META-LEARNING SPLITS:
â”œâ”€â”€ Meta-train classes: 10 attack types + Benign
â”œâ”€â”€ Meta-validation classes: 2 attack types (held out for hyperparameter tuning)
â”œâ”€â”€ Meta-test classes: 2 attack types (final evaluation of meta-learning)
â””â”€â”€ Ensure Tier 1 classes are in meta-test for honest evaluation
```

---

# Phase 2: Edge Candidate Generation

## 2.1 Edge Type Definitions

Define K=6 edge types, each with distinct semantic meaning:

```
EDGE TYPE 1: TEMPORAL SEQUENTIAL
â”œâ”€â”€ Definition: Flows in temporal sequence within same communication pair
â”œâ”€â”€ Condition: same (src_ip, dst_ip) AND time_gap < 60 seconds
â”œâ”€â”€ Semantics: Captures session continuity, multi-flow attacks
â”œâ”€â”€ Expected density: High for brute force, SSH attacks
â””â”€â”€ Directionality: Directed (earlier â†’ later)

EDGE TYPE 2: TEMPORAL CO-OCCURRENCE
â”œâ”€â”€ Definition: Flows occurring in same time window
â”œâ”€â”€ Condition: |start_time_u - start_time_v| < 1 second
â”œâ”€â”€ Semantics: Captures coordinated attacks, bursts
â”œâ”€â”€ Expected density: Very high for DDoS, flooding
â””â”€â”€ Directionality: Undirected

EDGE TYPE 3: SAME DESTINATION
â”œâ”€â”€ Definition: Flows targeting the same destination
â”œâ”€â”€ Condition: same dst_ip AND time_gap < 10 seconds
â”œâ”€â”€ Semantics: Captures attack targets, victim clustering
â”œâ”€â”€ Expected density: High for DDoS, all attack types
â””â”€â”€ Directionality: Undirected

EDGE TYPE 4: SAME SOURCE
â”œâ”€â”€ Definition: Flows originating from same source
â”œâ”€â”€ Condition: same src_ip AND time_gap < 10 seconds
â”œâ”€â”€ Semantics: Captures attacker behavior patterns
â”œâ”€â”€ Expected density: High for scanning, bot behavior
â””â”€â”€ Directionality: Undirected

EDGE TYPE 5: BEHAVIORAL SIMILARITY
â”œâ”€â”€ Definition: Flows with similar traffic fingerprint
â”œâ”€â”€ Condition: cosine_similarity(behavioral_vector_u, behavioral_vector_v) > threshold
â”œâ”€â”€ Behavioral vector: [bytes_ratio, pkt_ratio, duration_log, iat_cv, tcp_flag_hash]
â”œâ”€â”€ Semantics: Captures attack signatures, tool fingerprints
â”œâ”€â”€ Expected density: Medium, attack-type specific
â””â”€â”€ Directionality: Undirected

EDGE TYPE 6: CAUSAL/RESPONSE
â”œâ”€â”€ Definition: Request-response pairs or causal chains
â”œâ”€â”€ Condition: (src_u, dst_u) == (dst_v, src_v) AND start_v > end_u AND gap < 5s
â”œâ”€â”€ Alternative: DNS query followed by connection to resolved IP
â”œâ”€â”€ Semantics: Captures bidirectional communication, C2 patterns
â”œâ”€â”€ Expected density: Medium for infiltration, C2
â””â”€â”€ Directionality: Directed (cause â†’ effect)
```

## 2.2 Candidate Generation Strategy

```
HIERARCHICAL CANDIDATE GENERATION:

Level 1: Coarse filtering (rule-based, CPU, fast)
â”œâ”€â”€ For each flow, identify potential neighbors using:
â”‚   â”œâ”€â”€ IP-based: Hash table lookup for same src/dst IP
â”‚   â”œâ”€â”€ Temporal: Sliding window index for time proximity
â”‚   â””â”€â”€ Output: ~500-1000 candidates per flow

Level 2: Fine filtering (feature-based, CPU, medium)
â”œâ”€â”€ Apply edge type conditions
â”œâ”€â”€ Compute relation features for surviving candidates
â”œâ”€â”€ Output: ~100-200 candidates per flow per edge type

Level 3: Neural scoring (EPN, GPU, expensive)
â”œâ”€â”€ Score all Level 2 candidates with Edge Proposal Network
â”œâ”€â”€ Keep top-k based on learned scores
â”œâ”€â”€ Output: Final edge set based on budget allocation

COMPUTATIONAL OPTIMIZATION:
â”œâ”€â”€ Build inverted indices for IP addresses (hash maps)
â”œâ”€â”€ Build temporal index (sorted by start_time, binary search)
â”œâ”€â”€ Use Locality-Sensitive Hashing for behavioral similarity
â”œâ”€â”€ Batch GPU processing: 100K candidates per batch
â””â”€â”€ Estimated time: 4-6 hours for full 19M dataset on 8Ã—A100
```

## 2.3 Relation Features for EPN

For each candidate edge (u, v), compute relation features:

```
RELATION FEATURE VECTOR (input to EPN):

Temporal relations:
â”œâ”€â”€ time_diff: |start_time_u - start_time_v| (log-scaled)
â”œâ”€â”€ overlap: max(0, min(end_u, end_v) - max(start_u, start_v))
â”œâ”€â”€ temporal_order: 1 if u before v, -1 if after, 0 if overlap
â””â”€â”€ same_hour: 1 if same hour_of_day

Network relations:
â”œâ”€â”€ same_src_ip: binary
â”œâ”€â”€ same_dst_ip: binary
â”œâ”€â”€ same_src_subnet: binary (/24 match)
â”œâ”€â”€ same_dst_subnet: binary (/24 match)
â”œâ”€â”€ same_5tuple: binary
â”œâ”€â”€ is_bidirectional_pair: binary (reversed src/dst)
â””â”€â”€ port_similarity: jaccard(port_set_u, port_set_v)

Behavioral relations:
â”œâ”€â”€ bytes_ratio_diff: |log(bytes_u) - log(bytes_v)|
â”œâ”€â”€ duration_ratio: min(dur_u, dur_v) / max(dur_u, dur_v)
â”œâ”€â”€ feature_cosine: cosine_similarity(feature_u, feature_v)
â”œâ”€â”€ tcp_flag_match: hamming_distance(flags_u, flags_v)
â””â”€â”€ iat_pattern_similarity: DTW distance on IAT sequences

Label relations (training only):
â”œâ”€â”€ same_class: binary (1 if label_u == label_v)
â”œâ”€â”€ both_attack: binary (1 if both non-benign)
â””â”€â”€ class_pair_encoding: embedding of (class_u, class_v) pair

Total: ~25 relation features per candidate edge
```

---

# Phase 3: Meta-Learning for Edge Proposal Network (EPN)

## 3.1 EPN Architecture

```
EDGE PROPOSAL NETWORK:

Input Layer:
â”œâ”€â”€ Node u features: d_node dimensions (after PCA: 64)
â”œâ”€â”€ Node v features: d_node dimensions (64)
â”œâ”€â”€ Relation features: d_rel dimensions (25)
â””â”€â”€ Total input: 64 + 64 + 25 = 153

Architecture:
â”œâ”€â”€ Input projection:
â”‚   â”œâ”€â”€ node_u_proj = Linear(64, 128) + LayerNorm + GELU
â”‚   â”œâ”€â”€ node_v_proj = Linear(64, 128) + LayerNorm + GELU
â”‚   â””â”€â”€ rel_proj = Linear(25, 64) + LayerNorm + GELU
â”‚
â”œâ”€â”€ Cross-attention (optional, for learning node interactions):
â”‚   â”œâ”€â”€ Query: node_u_proj
â”‚   â”œâ”€â”€ Key/Value: node_v_proj
â”‚   â””â”€â”€ Output: attended_uv (128)
â”‚
â”œâ”€â”€ Fusion:
â”‚   â”œâ”€â”€ concat = [node_u_proj || node_v_proj || rel_proj || attended_uv]
â”‚   â””â”€â”€ Total: 128 + 128 + 64 + 128 = 448
â”‚
â”œâ”€â”€ MLP scoring head:
â”‚   â”œâ”€â”€ h1 = Linear(448, 256) + LayerNorm + GELU + Dropout(0.1)
â”‚   â”œâ”€â”€ h2 = Linear(256, 128) + LayerNorm + GELU + Dropout(0.1)
â”‚   â”œâ”€â”€ h3 = Linear(128, 64) + GELU
â”‚   â””â”€â”€ score = Sigmoid(Linear(64, 1))
â”‚
â””â”€â”€ Output: edge_score âˆˆ [0, 1]

Parameters: ~250K (lightweight, fast inference)
```

## 3.2 Meta-Learning Framework: Prototypical Edge Networks

```
PROTOTYPICAL EDGE NETWORK (PEN):

Core Idea:
â”œâ”€â”€ Learn edge selection that brings same-class nodes closer to class prototype
â”œâ”€â”€ Push different-class nodes away from each other
â””â”€â”€ Explicitly optimize for few-shot classification

Components:

1. FLOW ENCODER (shared with GNN):
   â”œâ”€â”€ Input: flow features x_i
   â”œâ”€â”€ Architecture: 2-layer MLP with residual connections
   â”‚   â”œâ”€â”€ h1 = x + Linear(ReLU(Linear(x, 256)), d) 
   â”‚   â””â”€â”€ h2 = h1 + Linear(ReLU(Linear(h1, 256)), d)
   â”œâ”€â”€ Output: embedding z_i âˆˆ â„^128
   â””â”€â”€ This encoder is shared with downstream GNN

2. PROTOTYPE COMPUTATION:
   â”œâ”€â”€ For each class c in episode:
   â”‚   â””â”€â”€ prototype_c = mean(z_i for i in support_set where y_i = c)
   â”œâ”€â”€ Prototypes represent class "centers" in embedding space
   â””â”€â”€ Updated every episode (not fixed)

3. EDGE SCORING WITH PROTOTYPES:
   â”œâ”€â”€ For candidate edge (u, v):
   â”‚   â”œâ”€â”€ If same predicted class:
   â”‚   â”‚   â””â”€â”€ score = Î± Ã— exp(-||z_u - z_v||Â² / Ï„)
   â”‚   â”œâ”€â”€ If different predicted class:
   â”‚   â”‚   â””â”€â”€ score = Î² Ã— exp(-||z_u - prototype_class(v)||Â² / Ï„)
   â”‚   â””â”€â”€ Combine with EPN score:
   â”‚       â””â”€â”€ final_score = Î³ Ã— EPN(u,v) + (1-Î³) Ã— prototype_score
   â””â”€â”€ Î±, Î², Î³, Ï„ are learnable parameters

4. EDGE SELECTION:
   â”œâ”€â”€ Apply Gumbel-Softmax for differentiable top-k selection
   â”œâ”€â”€ Temperature annealing: Ï„_gumbel starts at 1.0, decays to 0.1
   â””â”€â”€ Output: soft adjacency matrix A_soft
```

## 3.3 Episode Construction for Meta-Training

```
EPISODE SAMPLING STRATEGY:

Episode Structure:
â”œâ”€â”€ N-way: 5 classes per episode
â”œâ”€â”€ K-shot support: 10 samples per class
â”œâ”€â”€ K-shot query: 20 samples per class
â”œâ”€â”€ Background: 2000 samples from remaining classes
â””â”€â”€ Total nodes per episode: 5Ã—10 + 5Ã—20 + 2000 = 2150

Class Sampling Priority:
â”œâ”€â”€ Tier 1 (extreme minority): Appear in 80% of episodes
â”œâ”€â”€ Tier 2 (minority): Appear in 60% of episodes
â”œâ”€â”€ Tier 3 (moderate): Appear in 40% of episodes
â”œâ”€â”€ Tier 4 (common): Appear in 30% of episodes
â””â”€â”€ Tier 5 (benign): Appear in 50% of episodes (as background mostly)

Hard Negative Mining for Background:
â”œâ”€â”€ 30% of background: Random sampling
â”œâ”€â”€ 40% of background: Behaviorally similar to attack classes
â”‚   â””â”€â”€ Use LSH to find benign flows similar to attack support set
â”œâ”€â”€ 30% of background: Temporally adjacent to attack flows
â”‚   â””â”€â”€ Flows occurring within 10s of attack support set
â””â”€â”€ This prevents trivial edge patterns

Episode Edge Budget:
â”œâ”€â”€ Support set nodes: k_support = 100 edges each
â”œâ”€â”€ Query set nodes: k_query = 50 edges each
â”œâ”€â”€ Background nodes: k_background = 10 edges each
â””â”€â”€ Total edges per episode: ~150K (manageable for GPU)
```

## 3.4 Meta-Training Procedure

```
META-TRAINING ALGORITHM:

Hyperparameters:
â”œâ”€â”€ Meta-learning rate: Î±_meta = 1e-4
â”œâ”€â”€ Inner learning rate: Î±_inner = 1e-3
â”œâ”€â”€ Episodes per epoch: 100
â”œâ”€â”€ Epochs: 50
â”œâ”€â”€ Inner loop steps: 5 (for MAML-style, or 0 for pure Prototypical)
â””â”€â”€ Gradient accumulation: 4 episodes

For each meta-training epoch:
â”‚
â”œâ”€â”€ Sample 100 episodes (parallelized across 8 GPUs)
â”‚
â”œâ”€â”€ For each episode e:
â”‚   â”‚
â”‚   â”œâ”€â”€ 1. SAMPLE NODES
â”‚   â”‚   â”œâ”€â”€ support_set S = sample(N=5 classes, K=10 per class)
â”‚   â”‚   â”œâ”€â”€ query_set Q = sample(same N classes, K=20 per class)
â”‚   â”‚   â””â”€â”€ background B = sample_hard_negatives(2000)
â”‚   â”‚
â”‚   â”œâ”€â”€ 2. GENERATE CANDIDATE EDGES
â”‚   â”‚   â”œâ”€â”€ For all 6 edge types, generate candidates among S âˆª Q âˆª B
â”‚   â”‚   â””â”€â”€ ~500K candidates total per episode
â”‚   â”‚
â”‚   â”œâ”€â”€ 3. COMPUTE EDGE SCORES
â”‚   â”‚   â”œâ”€â”€ Encode all nodes: Z = FlowEncoder(S âˆª Q âˆª B)
â”‚   â”‚   â”œâ”€â”€ Compute prototypes: P_c = mean(Z[S_c]) for each class c
â”‚   â”‚   â”œâ”€â”€ Compute EPN scores: scores_epn = EPN(candidates, Z)
â”‚   â”‚   â”œâ”€â”€ Compute prototype scores: scores_proto = PrototypeScore(candidates, Z, P)
â”‚   â”‚   â””â”€â”€ Final: scores = Î³ Ã— scores_epn + (1-Î³) Ã— scores_proto
â”‚   â”‚
â”‚   â”œâ”€â”€ 4. SELECT EDGES (differentiable)
â”‚   â”‚   â”œâ”€â”€ Apply Gumbel-Softmax with temperature Ï„
â”‚   â”‚   â”œâ”€â”€ For support nodes: select top-100 per node
â”‚   â”‚   â”œâ”€â”€ For query nodes: select top-50 per node
â”‚   â”‚   â”œâ”€â”€ For background: select top-10 per node
â”‚   â”‚   â””â”€â”€ Build soft adjacency matrix A_soft
â”‚   â”‚
â”‚   â”œâ”€â”€ 5. RUN GNN ON EPISODE GRAPH
â”‚   â”‚   â”œâ”€â”€ GNN architecture: 3-layer GraphSAGE with mean aggregation
â”‚   â”‚   â”œâ”€â”€ Train on support set S with labels
â”‚   â”‚   â”œâ”€â”€ Inner loop: 5 gradient steps (MAML-style)
â”‚   â”‚   â”‚   â””â”€â”€ Or: 0 steps for pure Prototypical
â”‚   â”‚   â””â”€â”€ Obtain adapted parameters Î¸'_gnn
â”‚   â”‚
â”‚   â”œâ”€â”€ 6. EVALUATE ON QUERY SET
â”‚   â”‚   â”œâ”€â”€ predictions = GNN(Q; Î¸'_gnn)
â”‚   â”‚   â”œâ”€â”€ loss_ce = CrossEntropy(predictions, labels_Q)
â”‚   â”‚   â”œâ”€â”€ loss_focal = FocalLoss(predictions, labels_Q, Î³=2)  # For imbalance
â”‚   â”‚   â””â”€â”€ loss_episode = 0.5 Ã— loss_ce + 0.5 Ã— loss_focal
â”‚   â”‚
â”‚   â””â”€â”€ 7. COMPUTE META-GRADIENT
â”‚       â”œâ”€â”€ Backpropagate through:
â”‚       â”‚   GNN â†’ Gumbel-Softmax selection â†’ EPN scores â†’ EPN parameters
â”‚       â””â”€â”€ Accumulate gradient for EPN and FlowEncoder
â”‚
â”œâ”€â”€ After 4 episodes: Apply accumulated gradient
â”‚   â”œâ”€â”€ Update EPN parameters: Î¸_epn -= Î±_meta Ã— âˆ‡_epn
â”‚   â”œâ”€â”€ Update FlowEncoder: Î¸_enc -= Î±_meta Ã— âˆ‡_enc
â”‚   â””â”€â”€ Update Prototype parameters (Î±, Î², Î³, Ï„)
â”‚
â””â”€â”€ Validation every 10 epochs:
    â”œâ”€â”€ Run on held-out meta-validation classes
    â”œâ”€â”€ Compute rare class recall
    â”œâ”€â”€ Early stopping if no improvement for 5 validations
    â””â”€â”€ Save best checkpoint
```

## 3.5 Curriculum Meta-Learning

```
CURRICULUM STRATEGY:

Stage 1 (Epochs 1-15): Easy Episodes
â”œâ”€â”€ Class sampling: Only Tier 3-5 classes (larger sample sizes)
â”œâ”€â”€ Background: Random sampling (no hard negatives)
â”œâ”€â”€ Edge budget: Generous (2Ã— normal)
â””â”€â”€ Purpose: Learn basic edge patterns without extreme imbalance

Stage 2 (Epochs 16-30): Medium Episodes
â”œâ”€â”€ Class sampling: Include Tier 2 classes
â”œâ”€â”€ Background: 50% hard negatives
â”œâ”€â”€ Edge budget: Normal
â””â”€â”€ Purpose: Adapt to moderate imbalance

Stage 3 (Epochs 31-50): Hard Episodes
â”œâ”€â”€ Class sampling: Focus on Tier 1 classes (80% of episodes)
â”œâ”€â”€ Background: 70% hard negatives
â”œâ”€â”€ Edge budget: Class-adaptive (more for rare classes)
â”œâ”€â”€ Adversarial augmentation:
â”‚   â”œâ”€â”€ Random edge dropout (10%)
â”‚   â”œâ”€â”€ Feature noise injection (Ïƒ=0.1)
â”‚   â””â”€â”€ Temporal shuffle within 1-second windows
â””â”€â”€ Purpose: Robust edge selection for extreme minority classes
```

---

# Phase 4: Multi-Objective Optimization for Edge Budget Allocation

## 4.1 Decision Variables

```
BUDGET ALLOCATION VECTOR:

B = [B_1, B_2, B_3, B_4, B_5, B_6, B_class_1, ..., B_class_5]

Edge type budgets:
â”œâ”€â”€ B_1: Temporal sequential edges (in millions)
â”œâ”€â”€ B_2: Temporal co-occurrence edges (in millions)
â”œâ”€â”€ B_3: Same destination edges (in millions)
â”œâ”€â”€ B_4: Same source edges (in millions)
â”œâ”€â”€ B_5: Behavioral similarity edges (in millions)
â””â”€â”€ B_6: Causal/response edges (in millions)

Class-tier multipliers (how many edges per node in each tier):
â”œâ”€â”€ B_class_1: Multiplier for Tier 1 (extreme minority), range [1, 5]
â”œâ”€â”€ B_class_2: Multiplier for Tier 2 (minority), range [1, 3]
â”œâ”€â”€ B_class_3: Multiplier for Tier 3 (moderate), range [0.5, 2]
â”œâ”€â”€ B_class_4: Multiplier for Tier 4 (common), range [0.3, 1.5]
â””â”€â”€ B_class_5: Multiplier for Tier 5 (majority), range [0.1, 1]

Total decision variables: 11
Search space: Continuous, bounded
```

## 4.2 Objective Functions

```
OBJECTIVE 1: RARE CLASS MACRO-RECALL (MAXIMIZE)

fâ‚(B) = (1/|Rare|) Ã— Î£_{c âˆˆ Rare} Recall_c

Where:
â”œâ”€â”€ Rare = {SQL_Injection, XSS, Brute_Force_Web, LOIC-UDP, Slowloris}
â”œâ”€â”€ Recall_c = TP_c / (TP_c + FN_c)
â””â”€â”€ Computed on validation set after training GNN with budget B

Primary objective - directly measures what we care about most


OBJECTIVE 2: OVERALL MACRO-F1 (MAXIMIZE)

fâ‚‚(B) = (1/15) Ã— Î£_{c=1}^{15} F1_c

Where:
â”œâ”€â”€ F1_c = 2 Ã— (Precision_c Ã— Recall_c) / (Precision_c + Recall_c)
â””â”€â”€ Includes all 15 classes

Secondary objective - ensures we don't sacrifice common class performance


OBJECTIVE 3: GRAPH SPARSITY (MAXIMIZE, equivalent to minimizing edges)

fâ‚ƒ(B) = 1 - (|E(B)| / E_max)

Where:
â”œâ”€â”€ |E(B)| = total edges with budget allocation B
â”œâ”€â”€ E_max = maximum possible edges (e.g., 500M)
â””â”€â”€ Higher is better (sparser graph)

Tertiary objective - computational efficiency


OBJECTIVE 4: CLASS NEIGHBORHOOD PURITY (MAXIMIZE)

fâ‚„(B) = (1/N) Ã— Î£_v [1 - Entropy(class distribution in N(v))]

Where:
â”œâ”€â”€ N(v) = neighborhood of node v in graph
â”œâ”€â”€ Entropy = -Î£ p_c Ã— log(p_c) for class distribution
â””â”€â”€ Higher purity = neighbors mostly same class = better message passing

Auxiliary objective - graph structure quality


OBJECTIVE 5: RARE CLASS CONNECTIVITY (MAXIMIZE)

fâ‚…(B) = (1/|Rare|) Ã— Î£_{c âˆˆ Rare} AvgDegree(nodes in class c)

Where:
â”œâ”€â”€ AvgDegree = average number of edges per node
â””â”€â”€ Higher for rare classes = more information aggregation

Constraint-like objective - ensures rare classes aren't isolated
```

## 4.3 Constraints

```
HARD CONSTRAINTS:

1. Total edge budget:
   Î£ B_i â‰¤ B_total = 200M edges
   (Memory constraint: 200M edges â‰ˆ 15GB edge storage)

2. Minimum per-type budget:
   B_i â‰¥ 1M for all edge types
   (Ensures all edge types present for heterogeneous GNN)

3. Maximum per-type budget:
   B_i â‰¤ 100M for all edge types
   (Prevents single edge type domination)

4. Minimum rare class connectivity:
   For all c âˆˆ Tier 1: AvgDegree(c) â‰¥ 50
   (Ensures minimum information flow to rare classes)


SOFT CONSTRAINTS (as penalty terms):

1. Graph connectivity penalty:
   If num_connected_components > 1:
       penalty += 0.1 Ã— num_components

2. Class isolation penalty:
   For each class c:
       If AvgDegree(c) < 10:
           penalty += 0.05 Ã— (10 - AvgDegree(c))
```

## 4.4 Optimization Algorithm: NSGA-II with Surrogate

```
NSGA-II CONFIGURATION:

Population size: 100
Generations: 100
Crossover: SBX (Simulated Binary Crossover), Î·=15
Mutation: Polynomial mutation, Î·=20
Selection: Binary tournament based on rank + crowding distance


SURROGATE-ASSISTED OPTIMIZATION:

Phase A: Initial Sampling (expensive)
â”œâ”€â”€ Generate 200 random budget allocations (Latin Hypercube Sampling)
â”œâ”€â”€ For each allocation B:
â”‚   â”œâ”€â”€ Build graph with learned EPN and budget B
â”‚   â”œâ”€â”€ Sample 500K nodes for efficiency
â”‚   â”œâ”€â”€ Train GNN for 50 epochs
â”‚   â”œâ”€â”€ Compute all 5 objectives
â”‚   â””â”€â”€ Store (B, fâ‚, fâ‚‚, fâ‚ƒ, fâ‚„, fâ‚…)
â”œâ”€â”€ Parallelized across 8 GPUs: 200 evaluations / 8 = 25 per GPU
â””â”€â”€ Estimated time: 25 Ã— 30 min = 12.5 hours

Phase B: Surrogate Training
â”œâ”€â”€ Train XGBoost regressor for each objective
â”‚   â”œâ”€â”€ Input: B (11 features)
â”‚   â”œâ”€â”€ Output: predicted objective value
â”‚   â””â”€â”€ 5 separate models, one per objective
â”œâ”€â”€ Validation: 5-fold CV, ensure RÂ² > 0.8
â””â”€â”€ Uncertainty estimation: Ensemble of 10 XGBoost models

Phase C: Surrogate-Guided NSGA-II
â”œâ”€â”€ Run NSGA-II using surrogate for objective evaluation
â”œâ”€â”€ 100 generations Ã— 100 population = 10,000 evaluations
â”œâ”€â”€ All evaluations use surrogate (fast, <1 second each)
â””â”€â”€ Total time: ~3 hours

Phase D: Surrogate Refinement (active learning)
â”œâ”€â”€ Identify promising Pareto candidates from Phase C
â”œâ”€â”€ Select top-20 candidates with highest uncertainty
â”œâ”€â”€ Actually evaluate these 20 (expensive)
â”œâ”€â”€ Add to training set, retrain surrogate
â”œâ”€â”€ Repeat Phase C-D twice
â””â”€â”€ Additional time: 2 Ã— (20 Ã— 30 min / 8 GPUs) = 2.5 hours

Phase E: Final Pareto Frontier
â”œâ”€â”€ Collect all Pareto-optimal solutions
â”œâ”€â”€ Validate top-10 candidates with full training (100 epochs)
â”œâ”€â”€ Select final solution based on priority:
â”‚   â”œâ”€â”€ Primary filter: fâ‚ (rare recall) â‰¥ 0.7
â”‚   â”œâ”€â”€ Secondary filter: fâ‚‚ (macro-F1) â‰¥ 0.85
â”‚   â””â”€â”€ Tertiary selection: highest fâ‚ƒ (sparsest)
â””â”€â”€ Output: Optimal budget allocation B*

TOTAL OPTIMIZATION TIME: ~20 hours
```

## 4.5 Pareto Solution Selection

```
KNEE POINT DETECTION:

After obtaining Pareto frontier with ~50-100 solutions:

Method 1: Weighted Utopia Distance
â”œâ”€â”€ Normalize all objectives to [0, 1]
â”œâ”€â”€ Define utopia point: [max(fâ‚), max(fâ‚‚), max(fâ‚ƒ), max(fâ‚„), max(fâ‚…)]
â”œâ”€â”€ Define weights: w = [0.4, 0.25, 0.15, 0.1, 0.1]  # Prioritize rare recall
â”œâ”€â”€ Compute weighted distance: d_i = Î£ w_j Ã— (utopia_j - f_{ij})Â²
â””â”€â”€ Select: B* = argmin_i d_i

Method 2: Hierarchical Filtering
â”œâ”€â”€ Filter 1: Keep solutions where fâ‚ (rare recall) â‰¥ 0.65
â”œâ”€â”€ Filter 2: Among remaining, keep where fâ‚‚ (macro-F1) â‰¥ 0.80
â”œâ”€â”€ Filter 3: Among remaining, keep top-5 by fâ‚ƒ (sparsity)
â”œâ”€â”€ Final: Among top-5, select by domain preference
â””â”€â”€ Output: B* with guaranteed rare class performance

Method 3: Scenario-Based Selection
â”œâ”€â”€ Scenario A (Rare-class focus): Maximize fâ‚, ignore fâ‚ƒ
â”œâ”€â”€ Scenario B (Balanced): Use Method 1 with equal weights
â”œâ”€â”€ Scenario C (Efficient): Maximize fâ‚ƒ subject to fâ‚ â‰¥ 0.6, fâ‚‚ â‰¥ 0.8
â””â”€â”€ Report results for all three scenarios in paper
```

---

# Phase 5: Full Graph Construction

## 5.1 Applying Learned EPN to Full Dataset

```
FULL GRAPH CONSTRUCTION PIPELINE:

Input:
â”œâ”€â”€ Trained EPN with parameters Î¸*_epn
â”œâ”€â”€ Trained FlowEncoder with parameters Î¸*_enc
â”œâ”€â”€ Optimal budget allocation B*
â””â”€â”€ Full dataset: 19M flows

Step 1: Node Embedding (Parallel, 8 GPUs)
â”œâ”€â”€ Partition dataset into 8 chunks (~2.4M flows each)
â”œâ”€â”€ Each GPU computes: Z_chunk = FlowEncoder(X_chunk)
â”œâ”€â”€ Save embeddings to disk (memory-mapped)
â”œâ”€â”€ Time: ~30 minutes
â””â”€â”€ Storage: 19M Ã— 128 Ã— 4 bytes = 9.7 GB

Step 2: Candidate Generation (CPU, Parallel)
â”œâ”€â”€ For each edge type t:
â”‚   â”œâ”€â”€ Use inverted indices (IP hash tables, temporal index)
â”‚   â”œâ”€â”€ Generate candidates based on edge type rules
â”‚   â”œâ”€â”€ Compute relation features for each candidate
â”‚   â””â”€â”€ Store as sparse matrix
â”œâ”€â”€ Parallelize across 64 CPU cores
â”œâ”€â”€ Time: ~4 hours
â””â”€â”€ Storage: ~50GB temporary (sparse candidate matrices)

Step 3: EPN Scoring (GPU, Batched)
â”œâ”€â”€ For each edge type t:
â”‚   â”œâ”€â”€ Load candidate edges in batches of 1M
â”‚   â”œâ”€â”€ Load node embeddings (memory-mapped)
â”‚   â”œâ”€â”€ Compute: scores = EPN(Z[u], Z[v], relation_features)
â”‚   â””â”€â”€ Store scores
â”œâ”€â”€ Parallelize across 8 GPUs
â”œâ”€â”€ Time: ~2 hours
â””â”€â”€ Storage: ~20GB (score arrays)

Step 4: Class-Aware Edge Selection
â”œâ”€â”€ For each node v with class tier T:
â”‚   â”œâ”€â”€ Get multiplier m_T from B*
â”‚   â”œâ”€â”€ Base budget: k_base from B* for each edge type
â”‚   â”œâ”€â”€ Adjusted budget: k_v = k_base Ã— m_T
â”‚   â”œâ”€â”€ For each edge type t:
â”‚   â”‚   â”œâ”€â”€ Get top-k_v edges by EPN score
â”‚   â”‚   â””â”€â”€ Add to final edge set
â”‚   â””â”€â”€ Total edges for v: k_v Ã— 6 (edge types)
â”œâ”€â”€ Time: ~1 hour
â””â”€â”€ Output: Final edge list

Step 5: Graph Assembly
â”œâ”€â”€ Convert edge list to PyG/DGL format
â”œâ”€â”€ Add reverse edges for undirected types
â”œâ”€â”€ Compute edge type encodings
â”œâ”€â”€ Save final graph
â”œâ”€â”€ Time: ~30 minutes
â””â”€â”€ Storage: ~8GB (final graph)

TOTAL CONSTRUCTION TIME: ~8 hours
```

## 5.2 Edge Feature Engineering

```
EDGE FEATURES FOR HETEROGENEOUS GNN:

For each edge (u, v) of type t, compute:

Static edge features:
â”œâ”€â”€ edge_type_encoding: One-hot vector of length 6
â”œâ”€â”€ time_diff: |start_u - start_v| (log-scaled)
â”œâ”€â”€ feature_similarity: cosine(Z_u, Z_v)
â””â”€â”€ Total: 8 features

Dynamic edge features (computed during training):
â”œâ”€â”€ attention_weight: Learned by GAT layers
â”œâ”€â”€ message_importance: Gradient-based importance
â””â”€â”€ Used for interpretability, not stored

Edge weight initialization:
â”œâ”€â”€ weight = EPN_score Ã— (1 + 0.1 Ã— same_class_indicator)
â”œâ”€â”€ Normalized per-node: sum of outgoing weights = 1
â””â”€â”€ Used in message passing
```

---

# Phase 6: GNN Architecture and Training

## 6.1 Heterogeneous GNN Architecture

```
HETEROGENEOUS GRAPHSAGE WITH EDGE-TYPE ATTENTION:

Input:
â”œâ”€â”€ Node features: X âˆˆ â„^(N Ã— d_in), d_in = 100 (after feature engineering)
â”œâ”€â”€ Edge index: E = {E_1, ..., E_6} for 6 edge types
â”œâ”€â”€ Edge weights: W = {W_1, ..., W_6}
â””â”€â”€ N â‰ˆ 19M nodes

Layer Architecture (repeated L=4 times):

For each layer l:
â”‚
â”œâ”€â”€ 1. PER-TYPE MESSAGE COMPUTATION
â”‚   For each edge type t:
â”‚   â”‚   â”œâ”€â”€ Messages: M_t = W_t Ã— H^(l-1)[neighbors_t]
â”‚   â”‚   â”œâ”€â”€ Aggregation: A_t = MeanAggregator(M_t)
â”‚   â”‚   â””â”€â”€ Output: H_t^(l) âˆˆ â„^(N Ã— d_hidden)
â”‚   â””â”€â”€ Result: 6 aggregated representations per node
â”‚
â”œâ”€â”€ 2. EDGE-TYPE ATTENTION
â”‚   â”œâ”€â”€ Query: q = Linear(H^(l-1))
â”‚   â”œâ”€â”€ Keys: k_t = Linear(H_t^(l)) for each type t
â”‚   â”œâ”€â”€ Attention: Î±_t = Softmax(q Â· k_t / âˆšd)
â”‚   â””â”€â”€ Weighted combination: H_agg^(l) = Î£ Î±_t Ã— H_t^(l)
â”‚
â”œâ”€â”€ 3. SELF-LOOP AND UPDATE
â”‚   â”œâ”€â”€ Self: S = Linear(H^(l-1))
â”‚   â”œâ”€â”€ Combined: C = Concat(H_agg^(l), S)
â”‚   â”œâ”€â”€ Update: H^(l) = LayerNorm(ReLU(Linear(C)))
â”‚   â””â”€â”€ Residual: H^(l) = H^(l) + H^(l-1) (if same dim)
â”‚
â””â”€â”€ 4. DROPOUT
    â””â”€â”€ H^(l) = Dropout(H^(l), p=0.2)

Layer dimensions:
â”œâ”€â”€ Layer 1: 100 â†’ 256
â”œâ”€â”€ Layer 2: 256 â†’ 256
â”œâ”€â”€ Layer 3: 256 â†’ 128
â””â”€â”€ Layer 4: 128 â†’ 128

Classification Head:
â”œâ”€â”€ H_final = H^(4)
â”œâ”€â”€ Logits = Linear(128, 15)
â””â”€â”€ Probabilities = Softmax(Logits)

Total parameters: ~2M
```

## 6.2 Training Strategy

```
TRAINING CONFIGURATION:

Optimizer: AdamW
â”œâ”€â”€ Learning rate: 1e-3 with cosine annealing
â”œâ”€â”€ Weight decay: 1e-4
â”œâ”€â”€ Warmup: 1000 steps
â””â”€â”€ Total steps: 50,000

Batch sampling: NeighborSampler (mini-batch training)
â”œâ”€â”€ Batch size: 4096 target nodes
â”œâ”€â”€ Fanout: [15, 10, 10, 5] for 4 layers
â”œâ”€â”€ Per-edge-type fanout: Divide by 6, minimum 2
â””â”€â”€ Effective subgraph: ~200K nodes per batch

Loss function: Compound Loss
â”œâ”€â”€ L_ce: CrossEntropyLoss (standard)
â”œâ”€â”€ L_focal: FocalLoss with Î³=2 (for imbalance)
â”œâ”€â”€ L_center: CenterLoss (pull same-class embeddings together)
â”œâ”€â”€ L_contrastive: SupConLoss (supervised contrastive)
â””â”€â”€ L_total = 0.4Ã—L_ce + 0.3Ã—L_focal + 0.15Ã—L_center + 0.15Ã—L_contrastive

Class-balanced sampling:
â”œâ”€â”€ Oversample Tier 1-2 classes by 10Ã—
â”œâ”€â”€ Oversample Tier 3 classes by 3Ã—
â”œâ”€â”€ Undersample Tier 5 (benign) by 0.1Ã—
â””â”€â”€ Results in ~equal class representation per batch

Regularization:
â”œâ”€â”€ Dropout: 0.2 in GNN layers
â”œâ”€â”€ DropEdge: 0.1 (randomly drop edges during training)
â”œâ”€â”€ Label smoothing: 0.1
â””â”€â”€ Gradient clipping: max_norm=1.0

Mixed precision: FP16 training with GradScaler
â”œâ”€â”€ Reduces memory by ~40%
â”œâ”€â”€ Enables larger batch sizes
â””â”€â”€ Slight speedup

Distributed training: DistributedDataParallel across 8 GPUs
â”œâ”€â”€ Each GPU handles 1/8 of target nodes per batch
â”œâ”€â”€ Gradient synchronization every step
â”œâ”€â”€ Effective batch size: 4096 Ã— 8 = 32,768
â””â”€â”€ Training time: ~20 hours for 50K steps
```

## 6.3 Validation and Early Stopping

```
VALIDATION STRATEGY:

Validation frequency: Every 1000 steps

Validation metrics (computed on full validation set):
â”œâ”€â”€ Primary: Rare class macro-recall
â”œâ”€â”€ Secondary: Overall macro-F1
â”œâ”€â”€ Per-class: F1 for all 15 classes
â””â”€â”€ Confusion matrix

Early stopping:
â”œâ”€â”€ Patience: 10 validation rounds (10,000 steps)
â”œâ”€â”€ Monitor: Rare class macro-recall
â”œâ”€â”€ Restore best checkpoint
â””â”€â”€ Typical convergence: 30-40K steps

Checkpoint saving:
â”œâ”€â”€ Save every 5000 steps
â”œâ”€â”€ Keep top-3 by rare class recall
â”œâ”€â”€ Keep top-3 by overall F1
â””â”€â”€ Save final checkpoint
```

---

# Phase 7: Evaluation Framework

## 7.1 Metrics

```
PRIMARY METRICS (for paper/reporting):

1. Rare Class Macro-Recall
   â”œâ”€â”€ Classes: SQL_Injection, XSS, Brute_Force_Web, LOIC-UDP, Slowloris
   â”œâ”€â”€ Formula: (1/5) Ã— Î£ Recall_c
   â””â”€â”€ Target: â‰¥ 0.70

2. Overall Macro-F1
   â”œâ”€â”€ All 15 classes
   â”œâ”€â”€ Formula: (1/15) Ã— Î£ F1_c
   â””â”€â”€ Target: â‰¥ 0.85

3. Per-Class Metrics Table
   â”œâ”€â”€ Precision, Recall, F1 for each of 15 classes
   â””â”€â”€ Highlight Tier 1-2 classes

SECONDARY METRICS:

4. Weighted F1 (accounts for class sizes)
5. Cohen's Kappa (agreement beyond chance)
6. Matthews Correlation Coefficient (MCC)
7. Area Under ROC Curve (per-class and macro-average)
8. Area Under Precision-Recall Curve (especially for rare classes)

COMPUTATIONAL METRICS:

9. Graph size: Number of edges
10. Training time: Wall-clock hours
11. Inference time: Seconds per 10K flows
12. Memory usage: Peak GPU memory
```

## 7.2 Baseline Comparisons

```
BASELINE IMPLEMENTATIONS:

B1: No-Graph (MLP baseline)
â”œâ”€â”€ Architecture: 4-layer MLP on flow features
â”œâ”€â”€ Same training strategy (class balancing, loss function)
â”œâ”€â”€ Purpose: Quantify graph benefit

B2: Random k-NN Graph
â”œâ”€â”€ k=30 random neighbors per node
â”œâ”€â”€ Same GNN architecture
â”œâ”€â”€ Purpose: Quantify learned edge benefit

B3: Feature-Similarity Graph
â”œâ”€â”€ k=30 nearest neighbors by cosine similarity
â”œâ”€â”€ Same GNN architecture
â”œâ”€â”€ Purpose: Quantify meta-learning benefit

B4: Temporal-Only Graph
â”œâ”€â”€ Only temporal edges (types 1-2)
â”œâ”€â”€ Same GNN architecture
â”œâ”€â”€ Purpose: Quantify multi-edge-type benefit

B5: Heuristic Multi-Type Graph
â”œâ”€â”€ All 6 edge types, fixed ratio (equal budget)
â”œâ”€â”€ No EPN, just rule-based selection
â”œâ”€â”€ Purpose: Quantify EPN benefit

B6: Full Attention (GAT)
â”œâ”€â”€ Dense initial graph (k=50), GAT learns attention
â”œâ”€â”€ No meta-learning
â”œâ”€â”€ Purpose: Quantify meta-learning vs attention
```

## 7.3 Ablation Studies

```
ABLATION EXPERIMENTS:

A1: Meta-Learning Ablation
â”œâ”€â”€ Full system vs. EPN without meta-learning (random init)
â”œâ”€â”€ Measure: Rare class recall degradation

A2: Multi-Objective Ablation
â”œâ”€â”€ Full system vs. single-objective (accuracy only)
â”œâ”€â”€ Measure: Sparsity vs accuracy trade-off

A3: Class-Adaptive Density Ablation
â”œâ”€â”€ Full system vs. uniform edge density
â”œâ”€â”€ Measure: Rare class recall, per-class F1

A4: Edge Type Ablation
â”œâ”€â”€ Remove one edge type at a time
â”œâ”€â”€ Measure: Impact on different attack types
â”œâ”€â”€ Identify most important edge types per attack

A5: Prototype Component Ablation
â”œâ”€â”€ Full EPN vs. EPN without prototype scoring
â”œâ”€â”€ Measure: Few-shot performance

A6: Curriculum Ablation
â”œâ”€â”€ Full curriculum vs. no curriculum (random episodes)
â”œâ”€â”€ Measure: Training stability, final performance

A7: Loss Function Ablation
â”œâ”€â”€ Full loss vs. CE only
â”œâ”€â”€ Measure: Rare class recall impact
```

---

# Phase 8: Implementation Roadmap

## 8.1 Development Phases

```
PHASE 1: FOUNDATION (Weeks 1-2)
â”œâ”€â”€ Day 1-3: Data loading and preprocessing
â”‚   â”œâ”€â”€ Load 19M flows efficiently (Polars/Dask)
â”‚   â”œâ”€â”€ Implement feature engineering pipeline
â”‚   â””â”€â”€ Create train/val/test splits
â”œâ”€â”€ Day 4-7: Edge candidate generation
â”‚   â”œâ”€â”€ Implement 6 edge type generators
â”‚   â”œâ”€â”€ Build inverted indices for efficiency
â”‚   â””â”€â”€ Test on 100K subset
â”œâ”€â”€ Day 8-10: Basic GNN implementation
â”‚   â”œâ”€â”€ Implement heterogeneous GraphSAGE
â”‚   â”œâ”€â”€ Test on small graph (100K nodes)
â”‚   â””â”€â”€ Verify training loop works
â””â”€â”€ Day 11-14: Baseline implementations
    â”œâ”€â”€ Implement B1-B6 baselines
    â””â”€â”€ Run on 500K subset for sanity check

PHASE 2: META-LEARNING (Weeks 3-4)
â”œâ”€â”€ Day 15-18: EPN architecture
â”‚   â”œâ”€â”€ Implement Edge Proposal Network
â”‚   â”œâ”€â”€ Implement relation feature extraction
â”‚   â””â”€â”€ Test scoring on candidate edges
â”œâ”€â”€ Day 19-22: Episode sampling
â”‚   â”œâ”€â”€ Implement episode construction
â”‚   â”œâ”€â”€ Implement hard negative mining
â”‚   â””â”€â”€ Implement class-tier sampling
â”œâ”€â”€ Day 23-26: Meta-training loop
â”‚   â”œâ”€â”€ Implement Gumbel-Softmax selection
â”‚   â”œâ”€â”€ Implement meta-gradient computation
â”‚   â””â”€â”€ Test on small episodes
â””â”€â”€ Day 27-28: Curriculum and validation
    â”œâ”€â”€ Implement curriculum stages
    â””â”€â”€ Implement meta-validation

PHASE 3: MULTI-OBJECTIVE (Weeks 5-6)
â”œâ”€â”€ Day 29-32: Objective functions
â”‚   â”œâ”€â”€ Implement all 5 objectives
â”‚   â”œâ”€â”€ Implement constraint handling
â”‚   â””â”€â”€ Test computation on small graphs
â”œâ”€â”€ Day 33-36: NSGA-II implementation
â”‚   â”œâ”€â”€ Use pymoo library
â”‚   â”œâ”€â”€ Implement custom operators if needed
â”‚   â””â”€â”€ Test on synthetic objectives
â”œâ”€â”€ Day 37-40: Surrogate modeling
â”‚   â”œâ”€â”€ Implement XGBoost surrogate
â”‚   â”œâ”€â”€ Implement active learning loop
â”‚   â””â”€â”€ Validate surrogate accuracy
â””â”€â”€ Day 41-42: Integration testing
    â”œâ”€â”€ Run full optimization on 1M subset
    â””â”€â”€ Verify Pareto frontier quality

PHASE 4: SCALING (Weeks 7-8)
â”œâ”€â”€ Day 43-46: Full graph construction
â”‚   â”œâ”€â”€ Implement chunked processing
â”‚   â”œâ”€â”€ Implement memory-mapped storage
â”‚   â””â”€â”€ Test on full 19M dataset
â”œâ”€â”€ Day 47-50: Distributed training
â”‚   â”œâ”€â”€ Implement DistributedDataParallel
â”‚   â”œâ”€â”€ Implement efficient neighbor sampling
â”‚   â””â”€â”€ Profile and optimize
â”œâ”€â”€ Day 51-54: Full training runs
â”‚   â”œâ”€â”€ Train with optimal B*
â”‚   â”œâ”€â”€ Train baselines
â”‚   â””â”€â”€ Monitor and debug
â””â”€â”€ Day 55-56: Checkpoint management
    â”œâ”€â”€ Implement validation and selection
    â””â”€â”€ Save best models

PHASE 5: EVALUATION (Weeks 9-10)
â”œâ”€â”€ Day 57-60: Metric computation
â”‚   â”œâ”€â”€ Implement all metrics
â”‚   â”œâ”€â”€ Run on test set
â”‚   â””â”€â”€ Generate confusion matrices
â”œâ”€â”€ Day 61-64: Ablation studies
â”‚   â”œâ”€â”€ Run all ablation experiments
â”‚   â”œâ”€â”€ Collect results
â”‚   â””â”€â”€ Statistical significance tests
â”œâ”€â”€ Day 65-68: Analysis
â”‚   â”œâ”€â”€ Per-attack-type analysis
â”‚   â”œâ”€â”€ Edge type importance analysis
â”‚   â””â”€â”€ Visualization
â””â”€â”€ Day 69-70: Paper writing preparation
    â”œâ”€â”€ Tables and figures
    â””â”€â”€ Result documentation
```

## 8.2 Compute Budget

```
ESTIMATED COMPUTE USAGE:

Meta-Learning (Phase 2):
â”œâ”€â”€ Episodes: 5,000 (50 epochs Ã— 100 episodes)
â”œâ”€â”€ Time per episode: ~30 seconds on 1 GPU
â”œâ”€â”€ Parallelized across 8 GPUs
â”œâ”€â”€ Total: 5,000 Ã— 30s / 8 = ~5 hours
â””â”€â”€ Buffer for debugging: 20 hours

Multi-Objective (Phase 3):
â”œâ”€â”€ Initial sampling: 200 evaluations Ã— 30 min / 8 = 12.5 hours
â”œâ”€â”€ Surrogate optimization: ~3 hours
â”œâ”€â”€ Refinement: 2 Ã— 20 Ã— 30 min / 8 = 2.5 hours
â””â”€â”€ Total: ~20 hours

Full Graph Construction (Phase 4):
â”œâ”€â”€ Embedding computation: 30 minutes
â”œâ”€â”€ Candidate generation: 4 hours
â”œâ”€â”€ EPN scoring: 2 hours
â”œâ”€â”€ Edge selection: 1 hour
â”œâ”€â”€ Graph assembly: 30 minutes
â””â”€â”€ Total: ~8 hours

GNN Training (Phase 4):
â”œâ”€â”€ Main model: 50K steps Ã— 2s / 8 GPUs = 3.5 hours per run
â”œâ”€â”€ Multiple runs (hyperparameter tuning): 5 runs Ã— 3.5 = 17.5 hours
â””â”€â”€ Total: ~20 hours

Baselines and Ablations (Phase 5):
â”œâ”€â”€ 6 baselines Ã— 3.5 hours = 21 hours
â”œâ”€â”€ 7 ablations Ã— 3.5 hours = 24.5 hours
â””â”€â”€ Total: ~50 hours

GRAND TOTAL: ~125 GPU-hours on 8Ã—A100
Estimated cost: ~$400-500 (cloud pricing)
Wall-clock time: ~2 weeks with some parallelization
```

---

# Phase 9: Expected Outcomes and Success Criteria

## 9.1 Expected Performance

```
PERFORMANCE TARGETS:

Based on literature and preliminary analysis:

Rare Class Macro-Recall:
â”œâ”€â”€ Baseline (MLP): ~0.35-0.45
â”œâ”€â”€ Basic GNN (k-NN): ~0.50-0.60
â”œâ”€â”€ Our method: 0.70-0.80 (target: +20-30% improvement)
â””â”€â”€ Upper bound (if perfect): 1.0

Overall Macro-F1:
â”œâ”€â”€ Baseline (MLP): ~0.75-0.80
â”œâ”€â”€ Basic GNN (k-NN): ~0.82-0.86
â”œâ”€â”€ Our method: 0.87-0.92 (target: +5-10% improvement)
â””â”€â”€ Upper bound: ~0.95 (some inherent noise)

Per-Class Expected F1:
â”œâ”€â”€ Tier 1 (SQL Injection, XSS): 0.60-0.75 (significant improvement expected)
â”œâ”€â”€ Tier 2 (Brute_Force_Web, LOIC-UDP): 0.70-0.82
â”œâ”€â”€ Tier 3 (Slowloris, etc.): 0.80-0.88
â”œâ”€â”€ Tier 4 (Common attacks): 0.85-0.92
â””â”€â”€ Tier 5 (Benign): 0.95-0.98
```

## 9.2 Success Criteria

```
MINIMUM SUCCESS (paper-worthy):
â”œâ”€â”€ Rare class recall â‰¥ 0.65 (significant improvement over baselines)
â”œâ”€â”€ At least 3 ablations show statistically significant impact
â””â”€â”€ Clear Pareto trade-off demonstrated

TARGET SUCCESS (strong paper):
â”œâ”€â”€ Rare class recall â‰¥ 0.75
â”œâ”€â”€ Outperform all baselines on rare classes by â‰¥ 10%
â”œâ”€â”€ Demonstrate edge type importance varies by attack type
â””â”€â”€ Show meta-learning transfers to held-out attack classes

EXCEPTIONAL SUCCESS (top venue):
â”œâ”€â”€ Rare class recall â‰¥ 0.80
â”œâ”€â”€ State-of-the-art on CICIDS2018 benchmark
â”œâ”€â”€ Novel insights about attack graph structure
â””â”€â”€ Demonstrate zero-shot transfer to new attack types
```

## 9.3 Risk Mitigation

```
RISK 1: Meta-learning doesn't improve over heuristics
â”œâ”€â”€ Mitigation: Start with simpler prototypical approach
â”œâ”€â”€ Fallback: Use meta-learning for edge type weighting only
â””â”€â”€ Minimum viable: Show that class-adaptive density helps

RISK 2: Multi-objective optimization is too expensive
â”œâ”€â”€ Mitigation: Use surrogate modeling aggressively
â”œâ”€â”€ Fallback: Grid search over key budget ratios
â””â”€â”€ Minimum viable: Manual tuning with domain knowledge

RISK 3: Full graph doesn't fit in memory
â”œâ”€â”€ Mitigation: Chunked processing, memory-mapped storage
â”œâ”€â”€ Fallback: Subsample to 10M flows
â””â”€â”€ Minimum viable: Report results on largest feasible subset

RISK 4: Rare classes still underperform
â”œâ”€â”€ Mitigation: Increase class-adaptive density multipliers
â”œâ”€â”€ Additional: Use mixup/SMOTE for synthetic rare samples
â””â”€â”€ Fallback: Report honest results, analyze failure modes
```

---

# Summary

This pipeline integrates your meta-learning and multi-objective optimization research into a complete, implementable system. Key innovations:

1. **Prototypical Edge Networks:** Meta-learn edge selection optimized for few-shot rare class performance
2. **Class-Adaptive Graph Density:** Tier 1 classes get 20Ã— more edges than Tier 5
3. **Multi-Objective Budget Allocation:** Pareto-optimal trade-off between rare class recall, overall F1, and sparsity
4. **Heterogeneous Edge Types:** 6 semantic edge types with learned importance per attack type
5. **Curriculum Meta-Learning:** Progressive difficulty from easy to hard episodes

With 8Ã—A100 GPUs and ~2 weeks of compute, this should produce research-quality results suitable for a strong publication.

Shall I elaborate on any specific phase, or shall we start discussing implementation details for a particular component?