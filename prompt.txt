import polars as pl
import numpy as np
from scipy import stats
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, RobustScaler, 
    PowerTransformer, QuantileTransformer
)
from typing import Dict, Tuple, List, Optional
import warnings
warnings.filterwarnings('ignore')


class IndustryStandardScaler:
    """
    Ensures output meets industry standards:
    - Values ‚àà [-10, 10]
    - Mean ‚âà 0
    - Std ‚âà 1
    """
    def __init__(self, clip_range=(-10, 10), target_mean=0, target_std=1):
        self.clip_range = clip_range
        self.target_mean = target_mean
        self.target_std = target_std
        self.scaler = StandardScaler()
        self.original_mean = None
        self.original_std = None
        
    def fit_transform(self, X):
        """Fit and transform data to industry standard."""
        # Store original stats
        self.original_mean = np.mean(X)
        self.original_std = np.std(X)
        
        # Step 1: Standardize (mean=0, std=1)
        X_standardized = self.scaler.fit_transform(X)
        
        # Step 2: Clip to range [-10, 10]
        X_clipped = np.clip(X_standardized, self.clip_range[0], self.clip_range[1])
        
        # Step 3: Re-standardize to ensure mean‚âà0, std‚âà1 after clipping
        # (clipping can slightly affect these values)
        final_mean = np.mean(X_clipped)
        final_std = np.std(X_clipped)
        
        if final_std > 0:
            X_final = (X_clipped - final_mean) / final_std * self.target_std + self.target_mean
            # Ensure still within bounds after re-standardization
            X_final = np.clip(X_final, self.clip_range[0], self.clip_range[1])
        else:
            X_final = X_clipped
        
        return X_final
    
    def inverse_transform(self, X):
        """Approximate inverse transform."""
        # This is approximate because clipping loses information
        return self.scaler.inverse_transform(X)


class ColumnAnalyzer:
    """Analyzes column distributions and applies industry-standard transformations."""
    
    def __init__(self, df: pl.DataFrame, clip_range=(-10, 10)):
        self.df = df
        self.clip_range = clip_range
        self.numeric_cols = [col for col in df.columns if df[col].dtype in 
                            [pl.Int8, pl.Int16, pl.Int32, pl.Int64, 
                             pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
                             pl.Float32, pl.Float64]]
        self.scalers = {}  # Store fitted scalers for inverse transform
        self.transformations = {}  # Store transformation info
    
    def analyze_column(self, col: str) -> Dict:
        """Comprehensive analysis of a single column."""
        data = self.df[col].drop_nulls().to_numpy()
        
        if len(data) == 0:
            return {"error": "All values are null"}
        
        # Basic statistics
        stats_dict = {
            "column": col,
            "dtype": str(self.df[col].dtype),
            "count": len(data),
            "missing": self.df[col].null_count(),
            "mean": float(np.mean(data)),
            "median": float(np.median(data)),
            "std": float(np.std(data)),
            "min": float(np.min(data)),
            "max": float(np.max(data)),
            "q25": float(np.percentile(data, 25)),
            "q75": float(np.percentile(data, 75)),
        }
        
        # Distribution characteristics
        stats_dict["skewness"] = float(stats.skew(data))
        stats_dict["kurtosis"] = float(stats.kurtosis(data))
        
        # Outlier detection (IQR method)
        iqr = stats_dict["q75"] - stats_dict["q25"]
        lower_bound = stats_dict["q25"] - 1.5 * iqr
        upper_bound = stats_dict["q75"] + 1.5 * iqr
        outliers = np.sum((data < lower_bound) | (data > upper_bound))
        stats_dict["outlier_count"] = int(outliers)
        stats_dict["outlier_pct"] = float(outliers / len(data) * 100)
        
        # Range and scale
        stats_dict["range"] = stats_dict["max"] - stats_dict["min"]
        stats_dict["coefficient_of_variation"] = (
            stats_dict["std"] / abs(stats_dict["mean"]) 
            if stats_dict["mean"] != 0 else np.inf
        )
        
        # Normality tests
        if len(data) >= 8:
            _, p_shapiro = stats.shapiro(data[:5000])
            _, p_ks = stats.kstest(data, 'norm', args=(stats_dict["mean"], stats_dict["std"]))
            stats_dict["shapiro_p"] = float(p_shapiro)
            stats_dict["ks_p"] = float(p_ks)
            stats_dict["is_normal"] = (p_shapiro > 0.05) and (p_ks > 0.05)
        else:
            stats_dict["is_normal"] = False
        
        # Check for specific patterns
        stats_dict["has_zeros"] = bool(np.any(data == 0))
        stats_dict["has_negatives"] = bool(np.any(data < 0))
        stats_dict["unique_ratio"] = len(np.unique(data)) / len(data)
        
        return stats_dict
    
    def recommend_standardization(self, stats_dict: Dict) -> Dict:
        """Recommend transformation method (all output to industry standard)."""
        
        recommendations = {
            "column": stats_dict["column"],
            "primary_method": None,
            "pre_transform": None,  # Applied before industry standardization
            "reasoning": []
        }
        
        # Check for errors
        if "error" in stats_dict:
            recommendations["primary_method"] = "None - handle missing values first"
            return recommendations
        
        skew = abs(stats_dict["skewness"])
        outlier_pct = stats_dict["outlier_pct"]
        is_normal = stats_dict.get("is_normal", False)
        has_negatives = stats_dict["has_negatives"]
        has_zeros = stats_dict["has_zeros"]
        cv = stats_dict["coefficient_of_variation"]
        
        # All methods will output to industry standard, but we choose pre-processing
        
        # Low variance -> consider removing
        if cv < 0.1:
            recommendations["primary_method"] = "Consider removing (low variance)"
            recommendations["reasoning"].append(
                f"Very low coefficient of variation ({cv:.4f}) - may not be informative"
            )
            return recommendations
        
        # Highly skewed -> apply transformation first
        if skew > 2.0:
            if not has_negatives and not has_zeros:
                recommendations["pre_transform"] = "log"
                recommendations["reasoning"].append(
                    f"Very high skewness ({skew:.2f}) - log transform before standardization"
                )
            else:
                recommendations["pre_transform"] = "yeo-johnson"
                recommendations["reasoning"].append(
                    f"Very high skewness ({skew:.2f}) - Yeo-Johnson transform before standardization"
                )
        
        # Moderate skewness or outliers
        elif skew > 1.0 or outlier_pct > 5:
            recommendations["pre_transform"] = "robust"
            recommendations["reasoning"].append(
                f"Moderate skewness ({skew:.2f}) or outliers ({outlier_pct:.1f}%) - robust scaling first"
            )
        
        # Normal distribution or mild issues
        else:
            recommendations["pre_transform"] = "standard"
            recommendations["reasoning"].append(
                f"Relatively normal distribution - direct standardization"
            )
        
        # Final method always ensures industry standard
        recommendations["primary_method"] = "IndustryStandard"
        recommendations["reasoning"].append(
            f"Output guaranteed: Values ‚àà [{self.clip_range[0]}, {self.clip_range[1]}], Mean‚âà0, Std‚âà1"
        )
        
        return recommendations
    
    def apply_standardization(self, col: str, method: Optional[str] = None,
                            pre_transform: Optional[str] = None) -> np.ndarray:
        """
        Apply industry-standard transformation to a column.
        
        Parameters:
        -----------
        col : str
            Column name
        method : str, optional
            Transformation method (will always output to industry standard)
        pre_transform : str, optional
            Pre-transformation: 'log', 'yeo-johnson', 'robust', 'standard'
        """
        data = self.df[col].to_numpy().reshape(-1, 1)
        
        # Get recommendation if not specified
        if pre_transform is None:
            stats_dict = self.analyze_column(col)
            recommendation = self.recommend_standardization(stats_dict)
            
            if "Consider removing" in recommendation["primary_method"]:
                print(f"‚ö†Ô∏è  {col}: {recommendation['primary_method']}")
                return data.flatten()
            
            pre_transform = recommendation["pre_transform"]
        
        # Step 1: Pre-transformation (if needed)
        if pre_transform == "log":
            min_val = np.min(data)
            if min_val <= 0:
                shift = abs(min_val) + 1
                data_pre = np.log(data + shift)
                self.transformations[col] = {"pre": "log", "shift": shift}
            else:
                data_pre = np.log(data)
                self.transformations[col] = {"pre": "log", "shift": 0}
        
        elif pre_transform == "yeo-johnson":
            transformer = PowerTransformer(method='yeo-johnson')
            data_pre = transformer.fit_transform(data)
            self.transformations[col] = {"pre": "yeo-johnson", "transformer": transformer}
        
        elif pre_transform == "robust":
            robust_scaler = RobustScaler()
            data_pre = robust_scaler.fit_transform(data)
            self.transformations[col] = {"pre": "robust", "scaler": robust_scaler}
        
        else:  # standard or None
            data_pre = data
            self.transformations[col] = {"pre": "standard"}
        
        # Step 2: Apply industry standard transformation
        industry_scaler = IndustryStandardScaler(clip_range=self.clip_range)
        transformed = industry_scaler.fit_transform(data_pre)
        
        # Store scaler
        self.scalers[col] = industry_scaler
        self.transformations[col]["industry_scaler"] = industry_scaler
        
        return transformed.flatten()
    
    def transform_dataframe(self, custom_methods: Optional[Dict[str, str]] = None) -> pl.DataFrame:
        """
        Transform entire dataframe to industry standard.
        
        Parameters:
        -----------
        custom_methods : dict, optional
            Dictionary mapping column names to pre-transform methods:
            'log', 'yeo-johnson', 'robust', 'standard', or None for auto-detect
        
        Returns:
        --------
        Transformed dataframe with all values in industry standard format
        """
        df_transformed = self.df.clone()
        transformation_log = []
        
        print(f"\n{'='*80}")
        print(f"TRANSFORMING TO INDUSTRY STANDARD: Values ‚àà [{self.clip_range[0]}, {self.clip_range[1]}], Mean‚âà0, Std‚âà1")
        print(f"{'='*80}\n")
        
        for col in self.numeric_cols:
            # Get pre-transform method
            if custom_methods and col in custom_methods:
                pre_transform = custom_methods[col]
            else:
                stats_dict = self.analyze_column(col)
                recommendation = self.recommend_standardization(stats_dict)
                pre_transform = recommendation["pre_transform"]
            
            # Apply transformation
            print(f"üìä {col}: Pre-transform='{pre_transform}' ‚Üí Industry Standard")
            
            original_data = self.df[col].to_numpy()
            transformed_data = self.apply_standardization(col, pre_transform=pre_transform)
            
            # Update dataframe
            df_transformed = df_transformed.with_columns(
                pl.Series(name=col, values=transformed_data)
            )
            
            # Verify industry standard compliance
            t_mean = float(np.mean(transformed_data))
            t_std = float(np.std(transformed_data))
            t_min = float(np.min(transformed_data))
            t_max = float(np.max(transformed_data))
            
            # Check compliance
            in_range = (t_min >= self.clip_range[0]) and (t_max <= self.clip_range[1])
            mean_ok = abs(t_mean) < 0.1  # Allow small deviation
            std_ok = abs(t_std - 1.0) < 0.15  # Allow small deviation
            
            compliance = "‚úÖ" if (in_range and mean_ok and std_ok) else "‚ö†Ô∏è"
            
            transformation_log.append({
                "column": col,
                "pre_transform": pre_transform,
                "orig_mean": float(np.mean(original_data)),
                "orig_std": float(np.std(original_data)),
                "orig_min": float(np.min(original_data)),
                "orig_max": float(np.max(original_data)),
                "final_mean": t_mean,
                "final_std": t_std,
                "final_min": t_min,
                "final_max": t_max,
                "compliant": compliance
            })
        
        # Print summary
        print(f"\n{'='*80}")
        print("TRANSFORMATION SUMMARY")
        print(f"{'='*80}")
        log_df = pl.DataFrame(transformation_log)
        print(log_df.select([
            "column", "pre_transform", "orig_mean", "orig_std",
            "final_mean", "final_std", "final_min", "final_max", "compliant"
        ]))
        
        # Compliance check
        all_compliant = all(log["compliant"] == "‚úÖ" for log in transformation_log)
        print(f"\n{'='*80}")
        if all_compliant:
            print("‚úÖ ALL COLUMNS MEET INDUSTRY STANDARD!")
        else:
            print("‚ö†Ô∏è  Some columns may have minor deviations (check above)")
        print(f"{'='*80}\n")
        
        return df_transformed
    
    def analyze_all_columns(self) -> pl.DataFrame:
        """Analyze all numeric columns and return summary dataframe."""
        results = []
        
        for col in self.numeric_cols:
            stats_dict = self.analyze_column(col)
            recommendation = self.recommend_standardization(stats_dict)
            
            result = {
                **stats_dict,
                "pre_transform": recommendation["pre_transform"],
                "final_method": recommendation["primary_method"],
                "reasoning": " | ".join(recommendation["reasoning"])
            }
            results.append(result)
        
        return pl.DataFrame(results)
    
    def print_detailed_report(self, col: str):
        """Print detailed analysis for a specific column."""
        stats_dict = self.analyze_column(col)
        recommendation = self.recommend_standardization(stats_dict)
        
        print(f"\n{'='*70}")
        print(f"DETAILED ANALYSIS: {col}")
        print(f"{'='*70}")
        
        print(f"\nüìä Basic Statistics:")
        print(f"  Type: {stats_dict['dtype']}")
        print(f"  Count: {stats_dict['count']:,} | Missing: {stats_dict['missing']:,}")
        print(f"  Mean: {stats_dict['mean']:.4f} | Median: {stats_dict['median']:.4f}")
        print(f"  Std: {stats_dict['std']:.4f} | Range: [{stats_dict['min']:.4f}, {stats_dict['max']:.4f}]")
        
        print(f"\nüìà Distribution Characteristics:")
        print(f"  Skewness: {stats_dict['skewness']:.4f} | Kurtosis: {stats_dict['kurtosis']:.4f}")
        print(f"  CV: {stats_dict['coefficient_of_variation']:.4f}")
        print(f"  Outliers: {stats_dict['outlier_count']} ({stats_dict['outlier_pct']:.2f}%)")
        if 'is_normal' in stats_dict:
            print(f"  Normal Distribution: {'Yes' if stats_dict['is_normal'] else 'No'}")
        
        print(f"\nüéØ Recommended Transformation:")
        print(f"  PRE-TRANSFORM: {recommendation['pre_transform']}")
        print(f"  FINAL METHOD: {recommendation['primary_method']}")
        print(f"\n  Reasoning:")
        for reason in recommendation['reasoning']:
            print(f"    ‚Ä¢ {reason}")
        
        print(f"\n{'='*70}\n")
    
    def verify_industry_standard(self, df: pl.DataFrame, col: str) -> Dict:
        """Verify a column meets industry standards."""
        data = df[col].to_numpy()
        
        verification = {
            "column": col,
            "mean": float(np.mean(data)),
            "std": float(np.std(data)),
            "min": float(np.min(data)),
            "max": float(np.max(data)),
            "in_range": (np.min(data) >= self.clip_range[0]) and (np.max(data) <= self.clip_range[1]),
            "mean_centered": abs(np.mean(data)) < 0.1,
            "std_normalized": abs(np.std(data) - 1.0) < 0.15,
        }
        
        verification["compliant"] = all([
            verification["in_range"],
            verification["mean_centered"],
            verification["std_normalized"]
        ])
        
        return verification


# Example usage
if __name__ == "__main__":
    # Create sample dataframe with different distributions
    np.random.seed(42)
    
    df = pl.DataFrame({
        "normal_dist": np.random.normal(100, 15, 1000),
        "skewed_dist": np.random.exponential(2, 1000),
        "with_outliers": np.concatenate([
            np.random.normal(50, 10, 950),
            np.random.uniform(150, 200, 50)
        ]),
        "uniform_dist": np.random.uniform(0, 100, 1000),
        "log_normal": np.random.lognormal(3, 1, 1000),
        "wide_range": np.random.uniform(-1000, 5000, 1000),
    })
    
    # Initialize analyzer with industry standard range
    analyzer = ColumnAnalyzer(df, clip_range=(-10, 10))
    
    # Get summary for all columns
    print("ANALYSIS SUMMARY")
    print("="*100)
    summary = analyzer.analyze_all_columns()
    print(summary.select([
        "column", "mean", "std", "skewness", "outlier_pct", 
        "pre_transform", "final_method"
    ]))
    
    # Detailed report
    analyzer.print_detailed_report("skewed_dist")
    
    # Transform to industry standard
    df_transformed = analyzer.transform_dataframe()
    
    # Show results
    print("\nüìä ORIGINAL DATA (first 5 rows):")
    print(df.head())
    
    print("\n‚ú® INDUSTRY STANDARD DATA (first 5 rows):")
    print(df_transformed.head())
    
    # Verify compliance for each column
    print(f"\n{'='*80}")
    print("COMPLIANCE VERIFICATION")
    print(f"{'='*80}")
    
    for col in df_transformed.columns:
        verification = analyzer.verify_industry_standard(df_transformed, col)
        status = "‚úÖ COMPLIANT" if verification["compliant"] else "‚ùå NON-COMPLIANT"
        print(f"\n{col}: {status}")
        print(f"  Mean: {verification['mean']:.4f} (target: 0)")
        print(f"  Std:  {verification['std']:.4f} (target: 1)")
        print(f"  Range: [{verification['min']:.4f}, {verification['max']:.4f}] (target: [-10, 10])")
    
    # Custom pre-transforms example
    print(f"\n\n{'='*80}")
    print("CUSTOM PRE-TRANSFORMS EXAMPLE")
    print(f"{'='*80}\n")
    
    custom = {
        "normal_dist": "standard",
        "skewed_dist": "yeo-johnson",
        "with_outliers": "robust"
    }
    
    df_custom = analyzer.transform_dataframe(custom_methods=custom)
    
    # Export
    summary.write_csv("column_analysis_report.csv")
    df_transformed.write_csv("industry_standard_data.csv")
    print("\n‚úÖ Analysis exported to 'column_analysis_report.csv'")
    print("‚úÖ Transformed data exported to 'industry_standard_data.csv'")
